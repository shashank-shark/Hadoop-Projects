# FIRST INSTALL JAVA
yum install java-1.8.0-openjdk.x86_64 -y

# CREATE A USER CALLED HADOOP
# (RUN THE BELOW STEPS AS ROOT)
useradd hadoop
groupadd hadoopgrp
gpasswd -a hadoop hadoopgrp

# ENABLE SSH-PASSWORDLESS ACCESS TO HADOOP HOST
u - hadoop
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# VALIDATE SSH ACCESS
ssh [your_ip_here]

# DOWNLOAD THE LATEST VERSION OF HADOOP
wget http://apache.claz.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz
tar xzf hadoop-2.7.3-src.tar.gz
mv  hadoop-2.7.3/ /home/hadoop/hadoop

# SET UP THE HADOOP ENV VARIABLES
export HADOOP_HOME=/usr/local/lib/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

# APPLY THE CURRENT CHANGES IN THE ENVIRONMENT
source .bashrc

# EDIT THESE FILES

#(hadoop-env.sh)
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.101-3.b13.el6_8.x86_64

#(core-site.xml)
<configuration>
	<property>
  <name>fs.default.name</name>
      <value>hdfs://localhost:9000</value>
      </property>
</configuration>

#(hdfs-site.xml)
<configuration>
<property>
 <name>dfs.replication</name>
    <value>1</value>
</property>
 
<property>
  <name>dfs.name.dir</name>
          <value>file:/hdoop_store/hdfs/namenode</value>
 </property>
 
<property>
  <name>dfs.data.dir</name>
          <value>file:/hdoop_store/hdfs/datanode</value>
</property>
 
</configuration>


# MAKE SPACE TO RUN DATANODE AND NAMENODE
sudo mkdir -p /hdoop_store/hdfs/namenode
sudo mkdir -p /hadoop_store/hdfs/datanode
sudo chown -R hadoop:hadoopgrp /hadoop_store

# (mapred-site.xml)
<configuration>
<property>
  <name>mapred.job.tracker</name>
    <value>localhost:54311</value>
</property>
</configuration>


[------------------------------------------------]
# Now format the namenode
$] hadoop namenode -format
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
	
 
[hadoop@dhcppc4 hadoop]$ hadoop  namenode -format

which results in ----------------------->

DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
 
16/09/14 18:12:26 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hdp.domain.com/127.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.7.0
.........................
.........................
16/09/14 18:12:27 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
16/09/14 18:12:27 INFO util.GSet: capacity      = 2^15 = 32768 entries
16/09/14 18:12:27 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1656316296-127.0.0.1-1473840747357
16/09/14 18:12:27 INFO common.Storage: Storage directory /hadoop_store/hdfs/namenode has been successfully formatted.
16/09/14 18:12:27 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
16/09/14 18:12:27 INFO util.ExitUtil: Exiting with status 0
16/09/14 18:12:27 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hdp.domain.com/127.0.0.1
************************************************************/


# NOW START HADOOP
$]start-all.sh

which results in ------------------------------------------>

This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
16/09/14 18:17:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [localhost]
localhost: starting namenode, logging to /usr/local/lib/hadoop/logs/hadoop-hadoop-namenode-hdp.domain.com.out
localhost: starting datanode, logging to /usr/local/lib/hadoop/logs/hadoop-hadoop-datanode-hdp.domain.com.out
Starting secondary namenodes [0.0.0.0]
The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
RSA key fingerprint is 10:e4:4e:f7:3b:ab:ad:2c:d1:2d:8a:cb:22:61:82:ce.
Are you sure you want to continue connecting (yes/no)? yes
0.0.0.0: Warning: Permanently added '0.0.0.0' (RSA) to the list of known hosts.
0.0.0.0: starting secondarynamenode, logging to /usr/local/lib/hadoop/logs/hadoop-hadoop-secondarynamenode-hdp.domain.com.out
16/09/14 18:17:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
starting yarn daemons
starting resourcemanager, logging to /usr/local/lib/hadoop/logs/yarn-hadoop-resourcemanager-dhcppc4.out
localhost: starting nodemanager, logging to /usr/local/lib/hadoop/logs/yarn-hadoop-nodemanager-hdp.domain.com.out

# FIND THE PORT ON WHICH NODE IS RUNNING FROM THIS COMMAND
$]netstat -plten | grep java
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp               127.0.0.1:60818             0.0.0.0:*                   LISTEN      500        210996     19520/java
tcp               0.0.0.0:50070               0.0.0.0:*                   LISTEN      500        209866     19395/java
tcp               0.0.0.0:50010               0.0.0.0:*                   LISTEN      500        210990     19520/java
tcp               0.0.0.0:50075               0.0.0.0:*                   LISTEN      500        211211     19520/java
tcp               0.0.0.0:50020               0.0.0.0:*                   LISTEN      500        211214     19520/java
tcp               127.0.0.1:9000              0.0.0.0:*                   LISTEN      500        209881     19395/java
tcp               0.0.0.0:50090               0.0.0.0:*                   LISTEN      500        212636     19689/java

# NOW OPEN THE BROWER AND ACCESS WEB INTERFACE
